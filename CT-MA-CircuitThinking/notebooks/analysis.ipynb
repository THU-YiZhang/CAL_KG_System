{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CT-MA-CircuitThinking Analysis Notebook\n",
    "\n",
    "This notebook provides analysis and visualization tools for the CT-MA system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(str(Path.cwd().parent / \"src\"))\n",
    "\n",
    "from src.utils.config_manager import ConfigManager\n",
    "from src.utils.logger import setup_logger, get_logger\n",
    "from src.cot.format_validator import FormatValidator\n",
    "from src.cot.quality_checker import QualityChecker\n",
    "\n",
    "# Setup\n",
    "setup_logger()\n",
    "config = ConfigManager()\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "print(\"CT-MA Analysis Environment Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Generated CoT Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the latest CoT dataset\n",
    "cot_datasets_path = Path(config.get(\"data.cot_datasets_path\"))\n",
    "cot_files = list(cot_datasets_path.glob(\"cot_dataset_*.json\"))\n",
    "\n",
    "if cot_files:\n",
    "    latest_file = max(cot_files, key=lambda x: x.stat().st_mtime)\n",
    "    print(f\"Loading: {latest_file}\")\n",
    "    \n",
    "    with open(latest_file, 'r', encoding='utf-8') as f:\n",
    "        cot_data = json.load(f)\n",
    "    \n",
    "    print(f\"Loaded {len(cot_data)} CoT items\")\n",
    "else:\n",
    "    print(\"No CoT datasets found. Please run the generation first.\")\n",
    "    cot_data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cot_data:\n",
    "    # Filter successful items\n",
    "    successful_items = [item for item in cot_data if 'error' not in item]\n",
    "    \n",
    "    print(f\"Total items: {len(cot_data)}\")\n",
    "    print(f\"Successful items: {len(successful_items)}\")\n",
    "    print(f\"Success rate: {len(successful_items)/len(cot_data):.1%}\")\n",
    "    \n",
    "    if successful_items:\n",
    "        # Length statistics\n",
    "        logic_lengths = [len(item.get('logic', '')) for item in successful_items]\n",
    "        think_lengths = [len(item.get('think', '')) for item in successful_items]\n",
    "        answer_lengths = [len(item.get('answer', '')) for item in successful_items]\n",
    "        \n",
    "        print(f\"\\nAverage section lengths:\")\n",
    "        print(f\"  Logic: {sum(logic_lengths)/len(logic_lengths):.0f} chars\")\n",
    "        print(f\"  Think: {sum(think_lengths)/len(think_lengths):.0f} chars\")\n",
    "        print(f\"  Answer: {sum(answer_lengths)/len(answer_lengths):.0f} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Length Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if successful_items:\n",
    "    # Create DataFrame for analysis\n",
    "    df_data = []\n",
    "    for item in successful_items:\n",
    "        df_data.append({\n",
    "            'data_id': item.get('data_id', ''),\n",
    "            'application': item.get('source_application', ''),\n",
    "            'logic_length': len(item.get('logic', '')),\n",
    "            'think_length': len(item.get('think', '')),\n",
    "            'answer_length': len(item.get('answer', '')),\n",
    "            'total_length': len(item.get('logic', '')) + len(item.get('think', '')) + len(item.get('answer', ''))\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(df_data)\n",
    "    \n",
    "    # Plot length distributions\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Logic length distribution\n",
    "    axes[0, 0].hist(df['logic_length'], bins=20, alpha=0.7, color='blue')\n",
    "    axes[0, 0].set_title('Logic Section Length Distribution')\n",
    "    axes[0, 0].set_xlabel('Characters')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Think length distribution\n",
    "    axes[0, 1].hist(df['think_length'], bins=20, alpha=0.7, color='green')\n",
    "    axes[0, 1].set_title('Think Section Length Distribution')\n",
    "    axes[0, 1].set_xlabel('Characters')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    \n",
    "    # Answer length distribution\n",
    "    axes[1, 0].hist(df['answer_length'], bins=20, alpha=0.7, color='red')\n",
    "    axes[1, 0].set_title('Answer Section Length Distribution')\n",
    "    axes[1, 0].set_xlabel('Characters')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Total length distribution\n",
    "    axes[1, 1].hist(df['total_length'], bins=20, alpha=0.7, color='purple')\n",
    "    axes[1, 1].set_title('Total Length Distribution')\n",
    "    axes[1, 1].set_xlabel('Characters')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display summary statistics\n",
    "    print(\"\\nLength Statistics:\")\n",
    "    print(df[['logic_length', 'think_length', 'answer_length', 'total_length']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if successful_items:\n",
    "    # Analyze quality for a sample of items\n",
    "    quality_checker = QualityChecker(config)\n",
    "    \n",
    "    quality_scores = []\n",
    "    sample_size = min(10, len(successful_items))  # Analyze first 10 items\n",
    "    \n",
    "    print(f\"Analyzing quality for {sample_size} items...\")\n",
    "    \n",
    "    for i, item in enumerate(successful_items[:sample_size]):\n",
    "        quality_result = quality_checker.check_quality(item)\n",
    "        quality_scores.append({\n",
    "            'data_id': item.get('data_id', f'item_{i}'),\n",
    "            'overall_score': quality_result['overall_score'],\n",
    "            'logical_coherence': quality_result['dimension_scores']['logical_coherence']['score'],\n",
    "            'technical_accuracy': quality_result['dimension_scores']['technical_accuracy']['score'],\n",
    "            'reasoning_depth': quality_result['dimension_scores']['reasoning_depth']['score'],\n",
    "            'domain_relevance': quality_result['dimension_scores']['domain_relevance']['score'],\n",
    "            'completeness': quality_result['dimension_scores']['completeness']['score'],\n",
    "            'clarity': quality_result['dimension_scores']['clarity']['score']\n",
    "        })\n",
    "    \n",
    "    quality_df = pd.DataFrame(quality_scores)\n",
    "    \n",
    "    # Plot quality dimensions\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Radar chart for average quality dimensions\n",
    "    dimensions = ['logical_coherence', 'technical_accuracy', 'reasoning_depth', \n",
    "                 'domain_relevance', 'completeness', 'clarity']\n",
    "    avg_scores = [quality_df[dim].mean() for dim in dimensions]\n",
    "    \n",
    "    # Create radar chart\n",
    "    angles = [n / float(len(dimensions)) * 2 * 3.14159 for n in range(len(dimensions))]\n",
    "    angles += angles[:1]  # Complete the circle\n",
    "    avg_scores += avg_scores[:1]  # Complete the circle\n",
    "    \n",
    "    plt.subplot(111, projection='polar')\n",
    "    plt.plot(angles, avg_scores, 'o-', linewidth=2)\n",
    "    plt.fill(angles, avg_scores, alpha=0.25)\n",
    "    plt.xticks(angles[:-1], dimensions)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.title('Average Quality Dimensions')\n",
    "    plt.show()\n",
    "    \n",
    "    # Display quality statistics\n",
    "    print(\"\\nQuality Statistics:\")\n",
    "    print(quality_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sample CoT Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if successful_items:\n",
    "    # Display a sample CoT item\n",
    "    sample_item = successful_items[0]\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"SAMPLE COT ITEM\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Application: {sample_item.get('source_application', 'Unknown')}\")\n",
    "    print(f\"Data ID: {sample_item.get('data_id', 'Unknown')}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"LOGIC:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(sample_item.get('logic', 'No logic content')[:500] + \"...\")\n",
    "    print()\n",
    "    \n",
    "    print(\"THINK:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(sample_item.get('think', 'No think content')[:500] + \"...\")\n",
    "    print()\n",
    "    \n",
    "    print(\"ANSWER:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(sample_item.get('answer', 'No answer content'))\n",
    "    print()\n",
    "    \n",
    "    # Show metadata if available\n",
    "    if 'metadata' in sample_item:\n",
    "        print(\"METADATA:\")\n",
    "        print(\"-\" * 40)\n",
    "        for key, value in sample_item['metadata'].items():\n",
    "            print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export Analysis Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if successful_items:\n",
    "    # Export analysis results\n",
    "    reports_path = Path(config.get(\"data.reports_path\"))\n",
    "    reports_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Export length analysis\n",
    "    if 'df' in locals():\n",
    "        df.to_csv(reports_path / \"length_analysis.csv\", index=False)\n",
    "        print(f\"Length analysis exported to: {reports_path / 'length_analysis.csv'}\")\n",
    "    \n",
    "    # Export quality analysis\n",
    "    if 'quality_df' in locals():\n",
    "        quality_df.to_csv(reports_path / \"quality_analysis.csv\", index=False)\n",
    "        print(f\"Quality analysis exported to: {reports_path / 'quality_analysis.csv'}\")\n",
    "    \n",
    "    print(\"\\nAnalysis completed!\")\n",
    "else:\n",
    "    print(\"No data to analyze. Please generate CoT data first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
